type: plm
name: bert_lstm_crf
delimiter: "\t"
input: "resources/data/dataset/ner/zh/ccks/address/0621"
file_format: bies
output: "resources/data/output/ner/zh/ccks/address/0621/bert_lstm_crf/nezha-base-chinese"
summary: "resources/data/output/ner/zh/ccks/address/0621/bert_lstm_crf/nezha-base-chinese/summary"
max_length: 60
model_path: resources/data/pretrained/nezha-base-chinese
per_device_train_batch_size: 16
per_device_eval_batch_size: 8
plm_lr: 1.2220128677031903e-05
not_plm_lr: 0.00163381289736092
weight_decay: 0.0
model_type: bert
decode_type: general
num_train_epochs: 11
gradient_accumulation_steps: 1
lr_scheduler_type: linear
num_warmup_steps: 0
seed: 42
do_lower_case: True
task_name: ner
cpu: False

# plm:2.095e-5 not:0.00937,num:9,val:0.9234
# Trial 4 finished with value: 0.9248935738901277 and parameters: {'seed': 42, 'plm_lr': 1.2220128677031903e-05, 'not_plm_lr': 0.00163381289736092, 'num_train_epochs': 11}. Best is trial 4 with value: 0.9248935738901277.